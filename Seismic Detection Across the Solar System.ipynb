{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "def test_data_paths(data_dir, catalog_path):\n",
    "    \"\"\"Test if the paths exist and contain data\"\"\"\n",
    "    #print(\"\\nTesting data paths:\")\n",
    "    #print(f\"Data directory exists: {os.path.exists(data_dir)}\")\n",
    "    #print(f\"Catalog file exists: {os.path.exists(catalog_path)}\")\n",
    "    \n",
    "    if os.path.exists(data_dir):\n",
    "        files = os.listdir(data_dir)\n",
    "        #print(f\"Number of files in data directory: {len(files)}\")\n",
    "        #print(f\"First few files: {files[:5] if files else 'No files found'}\")\n",
    "    \n",
    "    if os.path.exists(catalog_path):\n",
    "        try:\n",
    "            catalog = pd.read_csv(catalog_path)\n",
    "            #print(f\"Catalog shape: {catalog.shape}\")\n",
    "            #print(f\"Catalog columns: {catalog.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading catalog: {str(e)}\")\n",
    "\n",
    "class MoonquakeClassifier:\n",
    "    def __init__(self, data_dir, catalog_path):\n",
    "        \"\"\"Initialize the classifier with paths to data directory and catalog file.\"\"\"\n",
    "        #print(\"\\n=== Initializing MoonquakeClassifier ===\")\n",
    "        self.data_dir = data_dir\n",
    "        #print(f\"Loading catalog from: {catalog_path}\")\n",
    "        self.catalog = pd.read_csv(catalog_path)\n",
    "        #print(f\"Catalog loaded successfully with shape: {self.catalog.shape}\")\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def extract_features(self, velocity_data, time_data):\n",
    "        \"\"\"Extract features from velocity time series data.\"\"\"\n",
    "        #print(\"Extracting features...\")\n",
    "        try:\n",
    "            # Time domain features\n",
    "            time_features = [\n",
    "                np.mean(velocity_data),\n",
    "                np.std(velocity_data),\n",
    "                np.max(np.abs(velocity_data)),\n",
    "                np.min(velocity_data),\n",
    "                np.percentile(velocity_data, 75),\n",
    "                np.percentile(velocity_data, 25),\n",
    "                np.mean(np.abs(np.diff(velocity_data))),\n",
    "                np.std(np.abs(np.diff(velocity_data)))\n",
    "            ]\n",
    "            \n",
    "            # Basic feature check\n",
    "            #print(f\"Time features extracted: {len(time_features)} features\")\n",
    "            \n",
    "            # Frequency domain features\n",
    "            time_step = time_data[1] - time_data[0]\n",
    "            fft_result = np.fft.fft(velocity_data)\n",
    "            frequencies = np.fft.fftfreq(len(velocity_data), d=time_step)\n",
    "            fft_magnitude = np.abs(fft_result)\n",
    "            \n",
    "            low_freq, high_freq = 0.1, 10\n",
    "            fft_filtered = fft_magnitude.copy()\n",
    "            fft_filtered[(frequencies < low_freq) | (frequencies > high_freq)] = 0\n",
    "            \n",
    "            freq_features = [\n",
    "                np.mean(fft_filtered),\n",
    "                np.std(fft_filtered),\n",
    "                np.max(fft_filtered),\n",
    "                np.sum(fft_filtered),\n",
    "                np.median(fft_filtered)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Frequency features extracted: {len(freq_features)} features\")\n",
    "            \n",
    "            # Spectral features\n",
    "            f, t, Sxx = signal.spectrogram(velocity_data, fs=1/time_step)\n",
    "            spectral_features = [\n",
    "                np.mean(Sxx),\n",
    "                np.std(Sxx),\n",
    "                np.max(Sxx)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Spectral features extracted: {len(spectral_features)} features\")\n",
    "            \n",
    "            all_features = time_features + freq_features + spectral_features\n",
    "            #print(f\"Total features extracted: {len(all_features)}\")\n",
    "            return all_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def process_single_file(self, file_path):\n",
    "        \"\"\"Process a single CSV file and extract features.\"\"\"\n",
    "        #print(f\"\\nProcessing file: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            #print(f\"File loaded successfully with shape: {df.shape}\")\n",
    "            \n",
    "            velocity = df['velocity(m/s)'].values\n",
    "            time_rel = df['time_rel(sec)'].values\n",
    "            #print(f\"Velocity data shape: {velocity.shape}\")\n",
    "            \n",
    "            features = self.extract_features(velocity, time_rel)\n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "    # def process_single_file(self, file_path):\n",
    "    #     print(f\"\\nProcessing file: {os.path.basename(file_path)}\")\n",
    "    #     try:\n",
    "    #         df = pd.read_csv(file_path)\n",
    "    #         print(f\"File loaded successfully with shape: {df.shape}\")\n",
    "\n",
    "    #         filename = os.path.basename(file_path)\n",
    "    #         # Extract relevant parts from filename (e.g., date, time prefix)\n",
    "    #         filename_parts = filename.split(\".\")  # Split on \".\"\n",
    "\n",
    "    #         # Modify this logic based on your actual filename format in the catalog\n",
    "    #         catalog_search_key = f\"{filename_parts[0]}.{filename_parts[1]}.{filename_parts[2]}\"\n",
    "\n",
    "    #         catalog_entry = self.catalog[self.catalog['filename'].str.contains(catalog_search_key)]\n",
    "\n",
    "    #         if len(catalog_entry) > 0:\n",
    "    #             print(f\"Found catalog entry for {filename}\")\n",
    "    #             features = self.extract_features(df['velocity(m/s)'].values, df['time_rel(sec)'].values)\n",
    "    #         if features is not None:\n",
    "    #             return features, catalog_entry['mq_type'].iloc[0]  # Return both features and class label\n",
    "    #         else:\n",
    "    #             print(f\"Error extracting features for {filename}\")\n",
    "    #         else:\n",
    "    #         print(f\"No matching catalog entry found for {filename}\")\n",
    "\n",
    "    #         return None, None  # If no match found, return None for both features and class label\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "    #         return None, None\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        Prepare the complete dataset by processing all files and matching with catalog.\n",
    "        Returns:\n",
    "            tuple: (X, y, processed_files) - features, labels, and list of processed filenames\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Preparing Dataset ===\")\n",
    "        X = []\n",
    "        y = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Get list of CSV files\n",
    "        csv_files = [f for f in os.listdir(self.data_dir) if f.endswith('.csv')]\n",
    "        #print(f\"Found {len(csv_files)} CSV files in directory\")\n",
    "        \n",
    "        # Debug: Print first few entries of the catalog\n",
    "        #print(\"\\nFirst few catalog entries:\")\n",
    "        #print(self.catalog['filename'].head())\n",
    "        \n",
    "        for filename in csv_files:\n",
    "            # Normalize filename (remove extension)\n",
    "            normalized_filename = filename.replace('.csv', '')\n",
    "            #print(f\"\\nProcessing {filename} (normalized: {normalized_filename})\")\n",
    "            \n",
    "            # Debug: Print exact matching condition\n",
    "            matching_entries = self.catalog[self.catalog['filename'].str.contains(normalized_filename, regex=False)]\n",
    "            #print(f\"Number of matching entries found: {len(matching_entries)}\")\n",
    "            \n",
    "            if len(matching_entries) > 0:\n",
    "                #print(f\"Found catalog entry for {filename}\")\n",
    "                file_path = os.path.join(self.data_dir, filename)\n",
    "                features = self.process_single_file(file_path)\n",
    "                \n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(matching_entries['mq_type'].iloc[0])\n",
    "                    processed_files.append(filename)\n",
    "                    #print(f\"Successfully processed {filename}\")\n",
    "            else:\n",
    "                print(f\"No catalog entry found for {filename}\")\n",
    "        \n",
    "        # Check if dataset is empty after processing all files\n",
    "        if not X:\n",
    "            print(\"No matching files found in the catalog. Dataset is empty.\")\n",
    "            return np.array([]), np.array([]), []\n",
    "        \n",
    "        # Convert to numpy arrays after processing all files\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # print(f\"\\nDataset preparation completed:\")\n",
    "        # print(f\"X shape: {X.shape}\")\n",
    "        # print(f\"y shape: {y.shape}\")\n",
    "        # print(f\"Processed {len(processed_files)} files successfully\")\n",
    "        \n",
    "        return X, y, processed_files\n",
    "            \n",
    "    # def prepare_dataset(self):\n",
    "    #     \"\"\"Prepare the complete dataset by processing all files and matching with catalog.\"\"\"\n",
    "    #     print(\"\\n=== Preparing Dataset ===\")\n",
    "    #     X = []\n",
    "    #     y = []\n",
    "    #     processed_files = []\n",
    "        \n",
    "    #     csv_files = [f for f in os.listdir(self.data_dir) if f.endswith('.csv')]\n",
    "    #     print(f\"Found {len(csv_files)} CSV files in directory\")\n",
    "        \n",
    "    #     for filename in csv_files:\n",
    "    #         print(f\"\\nProcessing {filename}\")\n",
    "    #         file_path = os.path.join(self.data_dir, filename)\n",
    "    #         catalog_entry = self.catalog[self.catalog['filename'] == filename]\n",
    "            \n",
    "    #         if len(catalog_entry) > 0:\n",
    "    #             print(f\"Found catalog entry for {filename}\")\n",
    "    #             features = self.process_single_file(file_path)\n",
    "    #             if features is not None:\n",
    "    #                 X.append(features)\n",
    "    #                 y.append(catalog_entry['mq_type'].iloc[0])\n",
    "    #                 processed_files.append(filename)\n",
    "    #                 print(f\"Successfully processed {filename}\")\n",
    "    #         else:\n",
    "    #             print(f\"No catalog entry found for {filename}\")\n",
    "        \n",
    "    #     X = np.array(X)\n",
    "    #     y = np.array(y)\n",
    "        \n",
    "    #     print(f\"\\nDataset preparation completed:\")\n",
    "    #     print(f\"X shape: {X.shape}\")\n",
    "    #     print(f\"y shape: {y.shape}\")\n",
    "    #     return X, y, processed_files\n",
    "    \n",
    "    def train_and_evaluate(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Train and evaluate the classifier.\"\"\"\n",
    "        print(\"\\n=== Training and Evaluation ===\")\n",
    "        try:\n",
    "            # Prepare dataset\n",
    "            X, y, processed_files = self.prepare_dataset()\n",
    "            \n",
    "            # Print dataset statistics\n",
    "            print(\"\\nDataset Statistics:\")\n",
    "            print(f\"Total samples: {len(X)}\")\n",
    "            print(\"\\nClass distribution:\")\n",
    "            unique_classes, counts = np.unique(y, return_counts=True)\n",
    "            for cls, count in zip(unique_classes, counts):\n",
    "                print(f\"{cls}: {count}\")\n",
    "            \n",
    "            # Encode labels\n",
    "            y_encoded = self.label_encoder.fit_transform(y)\n",
    "            print(f\"Encoded {len(unique_classes)} classes\")\n",
    "            \n",
    "            # Split dataset\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "            )\n",
    "            print(f\"\\nTrain set size: {len(X_train)}\")\n",
    "            print(f\"Test set size: {len(X_test)}\")\n",
    "            \n",
    "            # Scale features\n",
    "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "            X_test_scaled = self.scaler.transform(X_test)\n",
    "            \n",
    "            # Train and evaluate models\n",
    "            models = {\n",
    "                'Random Forest': RandomForestClassifier(\n",
    "                    n_estimators=100, \n",
    "                    max_depth=10,\n",
    "                    random_state=random_state\n",
    "                ),\n",
    "                'Neural Network': MLPClassifier(\n",
    "                    hidden_layer_sizes=(100, 50),\n",
    "                    max_iter=1000,\n",
    "                    random_state=random_state\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            results = {}\n",
    "            for name, model in models.items():\n",
    "                print(f\"\\nTraining {name}...\")\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                print(f\"{name} training completed\")\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                # Print classification report\n",
    "                print(f\"\\nClassification Report for {name}:\")\n",
    "                report = classification_report(\n",
    "                    y_test,\n",
    "                    y_pred,\n",
    "                    target_names=self.label_encoder.classes_\n",
    "                )\n",
    "                print(f\"\\n{report}\")\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'predictions': y_pred,\n",
    "                    'report': report\n",
    "                }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in training and evaluation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the moonquake classification pipeline.\"\"\"\n",
    "    try:\n",
    "        # Define paths\n",
    "        data_dir = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\data\\\\S12_GradeA'\n",
    "        catalog_path = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\catalogs\\\\apollo12_catalog_GradeA_final.csv'\n",
    "\n",
    "\n",
    "        \n",
    "        # Test paths before proceeding\n",
    "        # print(\"\\n=== Testing Data Paths ===\")\n",
    "        test_data_paths(data_dir, catalog_path)\n",
    "        \n",
    "        # Create instance of classifier\n",
    "        # print(\"\\n=== Creating Classifier Instance ===\")\n",
    "        classifier = MoonquakeClassifier(data_dir, catalog_path)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        # print(\"\\n=== Starting Training and Evaluation ===\")\n",
    "        results = classifier.train_and_evaluate()\n",
    "        \n",
    "        # Print final results\n",
    "        print(\"\\n=== Final Results ===\")\n",
    "        for model_name, model_results in results.items():\n",
    "            print(f\"\\nResults for {model_name}:\")\n",
    "            print(f\"Model: {model_results['model']}\")\n",
    "            print(f\"Classification Report:\\n{model_results['report']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred in main: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "if __name__ == \"main\":\n",
    "    print(\"\\n=== Starting Moonquake Classification Program ===\")\n",
    "    main()\n",
    "    print(\"\\n=== Program Completed ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training and Evaluation ===\n",
      "\n",
      "=== Preparing Dataset ===\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples: 76\n",
      "\n",
      "Class distribution:\n",
      "deep_mq: 9\n",
      "impact_mq: 64\n",
      "shallow_mq: 3\n",
      "Encoded 3 classes\n",
      "\n",
      "Train set size: 60\n",
      "Test set size: 16\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest training completed\n",
      "\n",
      "Classification Report for Random Forest:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.79      0.85      0.81        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69        16\n",
      "   macro avg       0.26      0.28      0.27        16\n",
      "weighted avg       0.64      0.69      0.66        16\n",
      "\n",
      "\n",
      "Training Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network training completed\n",
      "\n",
      "Classification Report for Neural Network:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.77      0.77      0.77        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62        16\n",
      "   macro avg       0.26      0.26      0.26        16\n",
      "weighted avg       0.62      0.62      0.62        16\n",
      "\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "Results for Random Forest:\n",
      "Model: RandomForestClassifier(max_depth=10, random_state=42)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.79      0.85      0.81        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69        16\n",
      "   macro avg       0.26      0.28      0.27        16\n",
      "weighted avg       0.64      0.69      0.66        16\n",
      "\n",
      "\n",
      "Results for Neural Network:\n",
      "Model: MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.77      0.77      0.77        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62        16\n",
      "   macro avg       0.26      0.26      0.26        16\n",
      "weighted avg       0.62      0.62      0.62        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Moonquake Classification Program ===\n",
      "\n",
      "=== Training and Evaluation ===\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples: 76\n",
      "\n",
      "Class distribution:\n",
      "deep_mq: 9\n",
      "impact_mq: 64\n",
      "shallow_mq: 3\n",
      "Encoded 3 classes\n",
      "\n",
      "Train set size: 60\n",
      "Test set size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Transformer Model...\n",
      "Epoch [10/50], Loss: 0.3658\n",
      "Epoch [20/50], Loss: 0.3958\n",
      "Epoch [30/50], Loss: 0.4663\n",
      "Epoch [40/50], Loss: 0.2943\n",
      "Epoch [50/50], Loss: 0.2659\n",
      "Training completed\n",
      "\n",
      "Classification Report for Transformer Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.81      1.00      0.90        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.81        16\n",
      "   macro avg       0.27      0.33      0.30        16\n",
      "weighted avg       0.66      0.81      0.73        16\n",
      "\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "Results for Transformer:\n",
      "Model: TransformerModel(\n",
      "  (embedding): Linear(in_features=16, out_features=64, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     deep_mq       0.00      0.00      0.00         2\n",
      "   impact_mq       0.81      1.00      0.90        13\n",
      "  shallow_mq       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.81        16\n",
      "   macro avg       0.27      0.33      0.30        16\n",
      "weighted avg       0.66      0.81      0.73        16\n",
      "\n",
      "\n",
      "=== Program Completed ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=64, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=128, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        return self.fc(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MoonquakeDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class MoonquakeClassifier:\n",
    "    def __init__(self, data_dir, catalog_path):\n",
    "        \"\"\"Initialize the classifier with paths to data directory and catalog file.\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.catalog = pd.read_csv(catalog_path)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def extract_features(self, velocity_data, time_data):\n",
    "        \"\"\"Extract features from velocity time series data.\"\"\"\n",
    "        #print(\"Extracting features...\")\n",
    "        try:\n",
    "            # Time domain features\n",
    "            time_features = [\n",
    "                np.mean(velocity_data),\n",
    "                np.std(velocity_data),\n",
    "                np.max(np.abs(velocity_data)),\n",
    "                np.min(velocity_data),\n",
    "                np.percentile(velocity_data, 75),\n",
    "                np.percentile(velocity_data, 25),\n",
    "                np.mean(np.abs(np.diff(velocity_data))),\n",
    "                np.std(np.abs(np.diff(velocity_data)))\n",
    "            ]\n",
    "            \n",
    "            # Basic feature check\n",
    "            #print(f\"Time features extracted: {len(time_features)} features\")\n",
    "            \n",
    "            # Frequency domain features\n",
    "            time_step = time_data[1] - time_data[0]\n",
    "            fft_result = np.fft.fft(velocity_data)\n",
    "            frequencies = np.fft.fftfreq(len(velocity_data), d=time_step)\n",
    "            fft_magnitude = np.abs(fft_result)\n",
    "            \n",
    "            low_freq, high_freq = 0.1, 10\n",
    "            fft_filtered = fft_magnitude.copy()\n",
    "            fft_filtered[(frequencies < low_freq) | (frequencies > high_freq)] = 0\n",
    "            \n",
    "            freq_features = [\n",
    "                np.mean(fft_filtered),\n",
    "                np.std(fft_filtered),\n",
    "                np.max(fft_filtered),\n",
    "                np.sum(fft_filtered),\n",
    "                np.median(fft_filtered)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Frequency features extracted: {len(freq_features)} features\")\n",
    "            \n",
    "            # Spectral features\n",
    "            f, t, Sxx = signal.spectrogram(velocity_data, fs=1/time_step)\n",
    "            spectral_features = [\n",
    "                np.mean(Sxx),\n",
    "                np.std(Sxx),\n",
    "                np.max(Sxx)\n",
    "            ]\n",
    "            \n",
    "            #print(f\"Spectral features extracted: {len(spectral_features)} features\")\n",
    "            \n",
    "            all_features = time_features + freq_features + spectral_features\n",
    "            #print(f\"Total features extracted: {len(all_features)}\")\n",
    "            return all_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def process_single_file(self, file_path):\n",
    "        \"\"\"Process a single CSV file and extract features.\"\"\"\n",
    "        #print(f\"\\nProcessing file: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            #print(f\"File loaded successfully with shape: {df.shape}\")\n",
    "            \n",
    "            velocity = df['velocity(m/s)'].values\n",
    "            time_rel = df['time_rel(sec)'].values\n",
    "            #print(f\"Velocity data shape: {velocity.shape}\")\n",
    "            \n",
    "            features = self.extract_features(velocity, time_rel)\n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        Prepare the complete dataset by processing all files and matching with catalog.\n",
    "        Returns:\n",
    "            tuple: (X, y, processed_files) - features, labels, and list of processed filenames\n",
    "        \"\"\"\n",
    "        # print(\"\\n=== Preparing Dataset ===\")\n",
    "        X = []\n",
    "        y = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Get list of CSV files\n",
    "        csv_files = [f for f in os.listdir(self.data_dir) if f.endswith('.csv')]\n",
    "        #print(f\"Found {len(csv_files)} CSV files in directory\")\n",
    "        \n",
    "        # Debug: Print first few entries of the catalog\n",
    "        #print(\"\\nFirst few catalog entries:\")\n",
    "        #print(self.catalog['filename'].head())\n",
    "        \n",
    "        for filename in csv_files:\n",
    "            # Normalize filename (remove extension)\n",
    "            normalized_filename = filename.replace('.csv', '')\n",
    "            #print(f\"\\nProcessing {filename} (normalized: {normalized_filename})\")\n",
    "            \n",
    "            # Debug: Print exact matching condition\n",
    "            matching_entries = self.catalog[self.catalog['filename'].str.contains(normalized_filename, regex=False)]\n",
    "            #print(f\"Number of matching entries found: {len(matching_entries)}\")\n",
    "            \n",
    "            if len(matching_entries) > 0:\n",
    "                #print(f\"Found catalog entry for {filename}\")\n",
    "                file_path = os.path.join(self.data_dir, filename)\n",
    "                features = self.process_single_file(file_path)\n",
    "                \n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(matching_entries['mq_type'].iloc[0])\n",
    "                    processed_files.append(filename)\n",
    "                    #print(f\"Successfully processed {filename}\")\n",
    "            else:\n",
    "                print(f\"No catalog entry found for {filename}\")\n",
    "        \n",
    "        # Check if dataset is empty after processing all files\n",
    "        if not X:\n",
    "            print(\"No matching files found in the catalog. Dataset is empty.\")\n",
    "            return np.array([]), np.array([]), []\n",
    "        \n",
    "        # Convert to numpy arrays after processing all files\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # print(f\"\\nDataset preparation completed:\")\n",
    "        # print(f\"X shape: {X.shape}\")\n",
    "        # print(f\"y shape: {y.shape}\")\n",
    "        # print(f\"Processed {len(processed_files)} files successfully\")\n",
    "        \n",
    "        return X, y, processed_files\n",
    "    def train_and_evaluate(self, test_size=0.2, random_state=42, batch_size=32, num_epochs=50):\n",
    "        print(\"\\n=== Training and Evaluation ===\")\n",
    "        try:\n",
    "            X, y, processed_files = self.prepare_dataset()\n",
    "            \n",
    "            print(\"\\nDataset Statistics:\")\n",
    "            print(f\"Total samples: {len(X)}\")\n",
    "            print(\"\\nClass distribution:\")\n",
    "            unique_classes, counts = np.unique(y, return_counts=True)\n",
    "            for cls, count in zip(unique_classes, counts):\n",
    "                print(f\"{cls}: {count}\")\n",
    "            \n",
    "            y_encoded = self.label_encoder.fit_transform(y)\n",
    "            print(f\"Encoded {len(unique_classes)} classes\")\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "            )\n",
    "            print(f\"\\nTrain set size: {len(X_train)}\")\n",
    "            print(f\"Test set size: {len(X_test)}\")\n",
    "            \n",
    "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "            X_test_scaled = self.scaler.transform(X_test)\n",
    "            \n",
    "            train_dataset = MoonquakeDataset(X_train_scaled, y_train)\n",
    "            test_dataset = MoonquakeDataset(X_test_scaled, y_test)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "            \n",
    "            model = TransformerModel(input_dim=X_train_scaled.shape[1], num_classes=len(unique_classes)).to(self.device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            \n",
    "            print(\"\\nTraining Transformer Model...\")\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                for batch_features, batch_labels in train_loader:\n",
    "                    batch_features, batch_labels = batch_features.to(self.device), batch_labels.to(self.device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_features.unsqueeze(1))  # Add sequence dimension\n",
    "                    loss = criterion(outputs, batch_labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            print(\"Training completed\")\n",
    "            \n",
    "            model.eval()\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for batch_features, batch_labels in test_loader:\n",
    "                    batch_features, batch_labels = batch_features.to(self.device), batch_labels.to(self.device)\n",
    "                    outputs = model(batch_features.unsqueeze(1))\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(batch_labels.cpu().numpy())\n",
    "            \n",
    "            print(\"\\nClassification Report for Transformer Model:\")\n",
    "            report = classification_report(\n",
    "                all_labels,\n",
    "                all_preds,\n",
    "                target_names=self.label_encoder.classes_\n",
    "            )\n",
    "            print(report)\n",
    "            \n",
    "            return {'Transformer': {'model': model, 'report': report}}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in training and evaluation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the moonquake classification pipeline.\"\"\"\n",
    "    # Define paths\n",
    "    data_dir = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\data\\\\S12_GradeA'\n",
    "    catalog_path = 'C:\\\\Users\\\\DELL\\\\Desktop\\\\New folder\\\\space_apps_2024_seismic_detection\\\\data\\\\lunar\\\\training\\\\catalogs\\\\apollo12_catalog_GradeA_final.csv'\n",
    "    \n",
    "    classifier = MoonquakeClassifier(data_dir, catalog_path)\n",
    "    results = classifier.train_and_evaluate()\n",
    "    \n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    for model_name, model_results in results.items():\n",
    "        print(f\"\\nResults for {model_name}:\")\n",
    "        print(f\"Model: {model_results['model']}\")\n",
    "        print(f\"Classification Report:\\n{model_results['report']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== Starting Moonquake Classification Program ===\")\n",
    "    main()\n",
    "    print(\"\\n=== Program Completed ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
